{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "college_name = ''\n",
    "course_name = ''\n",
    "year = 19 \n",
    "\n",
    "# Year should just be the last 2 digits of the joining year\n",
    "# college_name & course_name must be EXACT strings from \n",
    "# college_codes.csv and course_codes.csv\n",
    "\n",
    "# You can scrape college_codes.csv & course_codes.csv separately first\n",
    "# Find the relevant college_name and course_name, paste it here before starting.\n",
    "\n",
    "# A sample would look like this:\n",
    "\n",
    "# college_name = \"Faculty of Management Studies\"\n",
    "# course_name = \"(P.G)- MASTER OF BUSINESS ADMINISTRATION (FULL TIME) (M.B.A)\"\n",
    "# year = 19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DUScraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script scrapes the DU result website to extract student result data.  \n",
    "I wrote this to practice scraping - the website is a decent challenge for a beginner as it contains a form that needs to be filled as well as a captcha that you need to pass in order to access the student results.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All DU roll numbers look like this:\n",
    "\n",
    "YYCC01CC1IND\n",
    "\n",
    "Breaking it down into 4 components:  \n",
    "__YY-CC0-CC1-IND__\n",
    "\n",
    "- __YY__: Last 2 digits of joining year. For eg, 18.\n",
    "- __CC0__: College code. This is found in the source code of the form for accessing the result, stored in college_codes_page link given below. These are generally 3 digit codes, like 036 or 027.\n",
    "- __CC1__: Course code. This is found in the course_codes_page link given below. These are also generally 3 digit codes, like 510, 501 and so on.\n",
    "- __IND__: Index number. These start at 001 and go up till 999(?) at most."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check for paths\n",
    "import os\n",
    "\n",
    "# Web crawler\n",
    "from splinter import Browser #web crawler\n",
    "\n",
    "# For handling data\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "# For plotting graphs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from time import sleep\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters, links etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser_used = 'chrome'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "college_codes_page = 'http://durslt.du.ac.in/DURSLT_ND2020/Students/Combine_GradeCard.aspx'\n",
    "course_codes_page = 'http://durslt.du.ac.in/DURSLT_ND2020/Students/List_Of_Declared_Results.aspx'\n",
    "result_page = college_codes_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The script will go to sleep for halt_time seconds after every \n",
    "# halt_after number of records are extracted. This is to avoid detection\n",
    "\n",
    "halt_time = 30\n",
    "halt_after = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't edit this unless the error msg changes on the DU website\n",
    "error_msg = 'Sorry! no record found.' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The script will allow p_errors_allowed back to back missing student records \n",
    "# to be checked before it concludes that the roll numbers have terminated.\n",
    "# Edit according to common sense.\n",
    "p_errors_allowed = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_at = 1 # Roll number 1\n",
    "end_at = 20\n",
    "\n",
    "# If you want the entire batch results, set end_at to 999\n",
    "# Else, set it to a lower number for a sample.\n",
    "# Note that it's possible there may be missing results in between!\n",
    "\n",
    "# Note that the program will not check all between start_at and end_at. \n",
    "# In the middle, if it faces errors (missing results) repeatedly back to back more than \n",
    "# or equal to p_errors_allowed times, then it will consider that the roll numbers have \n",
    "# ended so end_at only defines an upper limit, \n",
    "# not the actual times the script will run to extract the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-requisites (college codes & course codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THIS_FOLDER = os.path.abspath('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prereq_data = True # flag var, will be set to False if the data is not there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists( os.path.join(THIS_FOLDER, 'data/' )):\n",
    "    os.makedirs(os.path.join(THIS_FOLDER, 'data/' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists( os.path.join(THIS_FOLDER, 'prereq/' )):\n",
    "    os.makedirs(os.path.join(THIS_FOLDER, 'prereq/' ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### College codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note:__ All college codes will be scraped, regardless of whether their results are out yet or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = pd.DataFrame()\n",
    "\n",
    "try:\n",
    "    codes = pd.read_excel( os.path.join(THIS_FOLDER, 'prereq/college_codes.xls'))\n",
    "    print(\"College codes loaded!\")\n",
    "    \n",
    "except:\n",
    "    print(\"College codes not found!\")\n",
    "    print(\"Scraping college code data...\")\n",
    "    \n",
    "    browser = Browser(browser_used)\n",
    "    browser.visit(college_codes_page)\n",
    "\n",
    "    selection = browser.find_by_id('ddlcollege')\n",
    "    options = selection.find_by_tag('option')\n",
    "    \n",
    "    college_codes=dict()\n",
    "    for i in range(1, len(options)):\n",
    "        college_codes[ options[i].value] = options[i].text\n",
    "\n",
    "    col_codes_data = pd.DataFrame(list(college_codes.items()), index=college_codes.keys() )\n",
    "    col_codes_data.columns=['code', 'name']\n",
    "    col_codes_data.set_index(['code'], inplace=True)\n",
    "\n",
    "    browser.quit()\n",
    "    print(\"College codes scraped!\")\n",
    "    \n",
    "    col_codes_data.to_excel(os.path.join(THIS_FOLDER, 'prereq/college_codes.xls'))\n",
    "    codes = pd.read_excel(os.path.join(THIS_FOLDER, 'prereq/college_codes.xls'))\n",
    "    print(\"College codes loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Course Codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note:__ Only those course codes will be scraped whose results have been declared yet on the course_codes_page! If you're trying to scrape the results for a course that just got declared, you need to delete course_codes.xls first and then run the cell below to get the updated list of course_codes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "course_codes = pd.DataFrame()\n",
    "\n",
    "try:\n",
    "    course_codes = pd.read_excel(os.path.join(THIS_FOLDER, 'prereq/course_codes.xls'))\n",
    "    print(\"Course codes loaded!\")\n",
    "\n",
    "except:\n",
    "    print(\"Course codes not found!\")\n",
    "    print(\"Scraping course code data...\")\n",
    "    \n",
    "    browser = Browser(browser_used)\n",
    "    browser.visit(course_codes_page)\n",
    "    \n",
    "    data_result = browser.find_by_xpath(\"//table[@id='gvshow_Reg']//tbody//tr\")\n",
    "\n",
    "    data = list()\n",
    "    header = list()\n",
    "    \n",
    "    th = data_result[0].find_by_tag('th')\n",
    "    \n",
    "    for i in range(len(th)):\n",
    "        header.append(th[i].text)\n",
    "        \n",
    "    \n",
    "    for i in range(1, len(data_result)):\n",
    "        r_data = list()\n",
    "        td = data_result[i].find_by_tag('td')\n",
    "        for a in range(len(td)):\n",
    "            r_data.append(td[a].text)\n",
    "            \n",
    "        data.append(r_data)\n",
    "    \n",
    "    tempData = pd.DataFrame(data, columns = header)\n",
    "    \n",
    "    course_codes = tempData[['Course Code', 'Course Name']].drop_duplicates().set_index('Course Code')\n",
    "    browser.quit()\n",
    "    print(\"Course codes scraped!\")\n",
    "\n",
    "    \n",
    "    course_codes.to_excel(os.path.join(THIS_FOLDER, 'prereq/course_codes.xls'))\n",
    "    course_codes = pd.read_excel(os.path.join(THIS_FOLDER, 'prereq/course_codes.xls'))\n",
    "    print(\"Course codes loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "college_code = codes[ codes['name'] == college_name]['code'].values[0]\n",
    "course_code = course_codes[ course_codes['Course Name'] == course_name]['Course Code'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "students = [f\"{i:03d}\" for i in range(start_at, end_at + 1)] # 001,002,003 and so on...\n",
    "base_roll_no = f\"{year}{college_code}{course_code}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "which_dir = f\"data/{college_code}_{college_name}/{course_code}_{course_name}/20{year}\"\n",
    "base_dir = os.path.join(THIS_FOLDER, which_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(base_dir):\n",
    "    os.makedirs(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extracting raw data\n",
    "first = True # flag var, set to False after first result is found\n",
    "header = list()\n",
    "raw_data = pd.DataFrame()\n",
    "count = 0\n",
    "\n",
    "# The scraping won't begin if raw_data.xls exists in the base_dir\n",
    "execute = not (os.path.exists(os.path.join(base_dir, 'raw_data.xls')))\n",
    "\n",
    "if(execute):\n",
    "    p_errors=0\n",
    "\n",
    "    browser = Browser(browser_used)\n",
    "\n",
    "    for w in students:\n",
    "\n",
    "        roll_num = base_roll_no + w\n",
    "\n",
    "        browser.visit(result_page) # visit webpage\n",
    "        browser.find_by_id('ddlcollege').select(college_code) # select college\n",
    "\n",
    "        src = browser.evaluate_script(\"document.getElementById('imgCaptcha').src\")\n",
    "        captcha = src[ (src.find(\"Code=\") + 5) : (src.find(\"Code\") + 11) ]\n",
    "        # The captcha is a 6 digit number given in the link of the image itself\n",
    "        \n",
    "        browser.fill(\"txtcaptcha\", captcha)\n",
    "\n",
    "        browser.find_by_id('txtrollno').fill(str(roll_num)) # fill the roll number\n",
    "        browser.find_by_id(\"btnsearch\").click() # go\n",
    "\n",
    "        error = False\n",
    "        try:\n",
    "            error = (browser.find_by_id(\"lblmsg\").first.text == error_msg)\n",
    "        except:\n",
    "            print(f\"[{roll_num}] Processing.\")\n",
    "\n",
    "        if(error):\n",
    "            print(f\"[{roll_num}] Record not found.\")\n",
    "            p_errors += 1\n",
    "            if(p_errors == p_errors_allowed):\n",
    "                print(f\"[NOTE] Persistent errors have reached upper limit of p_errors_allowed = {p_errors_allowed}\")\n",
    "                print(\"[END] Terminating data collection...\")\n",
    "                break\n",
    "\n",
    "        else:\n",
    "            p_errors = 0 # p_errors reset if a result is found!\n",
    "            \n",
    "            data_rollno = browser.find_by_id(\"lblrollno\").first.text\n",
    "            data_name = browser.find_by_id(\"lblname\").first.text\n",
    "            data_result = browser.find_by_xpath(\"//table[@id='gvshow']//tbody//tr\")\n",
    "\n",
    "            data = list()\n",
    "\n",
    "            if(first):\n",
    "                header = list()\n",
    "                th = data_result[0].find_by_tag('th')\n",
    "                for i in range (len(th)):\n",
    "                    header.append(th[i].text)\n",
    "\n",
    "            for i in range(1, len(data_result)):\n",
    "                r_data = list()\n",
    "                td = data_result[i].find_by_tag('td')\n",
    "                for a in range(len(td)):\n",
    "                    r_data.append(td[a].text)\n",
    "                    \n",
    "                data.append(r_data)\n",
    "\n",
    "            tempData = pd.DataFrame(data, columns=header)\n",
    "            tempData.insert(0, \"RollNo\", [str(data_rollno) for i in range(0, len(data_result) - 1)])\n",
    "            tempData.insert(1, \"Name\", [str(data_name) for i in range(0, len(data_result) - 1)])\n",
    "            # len(data) - 1 because the first row is header row\n",
    "\n",
    "            raw_data = tempData if first else pd.concat([raw_data, tempData])\n",
    "            first = False\n",
    "\n",
    "            count += 1\n",
    "            if(count % halt_after == 0):\n",
    "                sleep(halt_time)\n",
    "\n",
    "    # Done scraping\n",
    "    raw_data.set_index([\"RollNo\", \"Name\"])\n",
    "    browser.quit()\n",
    "    print(\"Scraping over!\")\n",
    "    \n",
    "    if(count > 0):\n",
    "        raw_data.to_excel(os.path.join(base_dir, 'raw_data.xls'), sheet_name='raw')\n",
    "        print(\"Raw data exported\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DU results are presented differently depending on the course. The section of the script below has been tested for some undergrad data (in particular, 3 year CBCS courses) where data for each paper is presented in terms of a \"Grade Point\" between 0 to 10. Using this, the below section generates the following:\n",
    "- An Excel file which contains paperwise grades\n",
    "- An Excel file which contains the GPA\n",
    "- A PDF file which contains the grade distributions for all the papers in the raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note:__ There are certain courses for which the data is presented differently, so the section below won't work on that data without transforming it first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_excel(os.path.join(base_dir, 'raw_data.xlsx'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paperwise Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = list(np.unique( raw_data['Paper Name']))\n",
    "raw_data.set_index(['RollNo'], inplace=True)\n",
    "datasets = dict()\n",
    "sheet_count=0\n",
    "\n",
    "with pd.ExcelWriter(os.path.join(base_dir, 'paperwise.xls')) as writer:\n",
    "    for i in papers:\n",
    "        temp_data = raw_data[ raw_data['Paper Name'] == i][['Name', 'Grade Point']]\n",
    "        temp_data.columns = ['Name', i]\n",
    "        datasets[i] = temp_data\n",
    "        datasets[i].to_excel(writer, sheet_name=str(sheet_count))\n",
    "        sheet_count += 1\n",
    "        \n",
    "print(\"Paperwise data segregated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paperwise Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = 2\n",
    "rows = ceil(len(papers) / 2)\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "fig, ax = plt.subplots(rows, cols,figsize=(15, 5 * ceil(len(papers) / 2)))\n",
    "\n",
    "plot_count = 0\n",
    "workingData = 0\n",
    "\n",
    "for i in range(rows):\n",
    "    for j in range(cols):\n",
    "        if(plot_count == len(papers)):\n",
    "            break\n",
    "        \n",
    "        workingData = raw_data[ raw_data['Paper Name'] == papers[plot_count] ]['Grade Point']\n",
    "        ax[i, j].hist(workingData, bins=range(0,12), range=(0, 13))\n",
    "        ax[i,j].spines[\"top\"].set_visible(False)\n",
    "        ax[i,j].spines[\"right\"].set_visible(False)\n",
    "        ax[i,j].set_xlim(0, 12)\n",
    "        ax[i,j].set_ylim(0,  workingData.value_counts().max()+2)\n",
    "        ax[i,j].set_title(papers[plot_count])\n",
    "        ax[i,j].set_ylabel('Students')\n",
    "        ax[i,j].set_xlabel('Marks')\n",
    "        plot_count+=1\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "fig.savefig( os.path.join(base_dir, 'paperwise_distr.pdf'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPA Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_raw_data = raw_data.reset_index(drop=False)\n",
    "gpa_data = pd.DataFrame(columns=['RollNo', 'Name', 'GPA'])\n",
    "\n",
    "for k in students:\n",
    "    roll_no = base_roll_no + k\n",
    "    tmp_data = n_raw_data[ n_raw_data['RollNo'] == int(roll_no) ]\n",
    "    tmp_data.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    if(tmp_data.count().sum() != 0):\n",
    "        tmp_data = tmp_data.assign(wgrade = (tmp_data['Grade Point'] * tmp_data['Credit'] ))\n",
    "        name=tmp_data['Name'][0]\n",
    "        GPA= (tmp_data['wgrade'].sum()) / (tmp_data['Credit'].sum())\n",
    "\n",
    "        gpa_data = gpa_data.append( {'RollNo':roll_no, 'Name':name, 'GPA':GPA}, ignore_index=True)\n",
    "\n",
    "\n",
    "with pd.ExcelWriter(os.path.join(base_dir, 'gpa.xls')) as writer:\n",
    "    gpa_data.to_excel(writer, sheet_name='unsorted')\n",
    "    gpa_data.sort_values(by = ['GPA'], ascending=False).reset_index(drop=True).to_excel(writer, sheet_name='sorted')\n",
    "    \n",
    "print(\"GPA calculated!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
